— pattern = r"[Pp]ython"
python_counts = titles.str.contains(pattern).sum()
print(python_counts)
												         iki kod ayni seyi yapiyor
pattern = r"python"
python_counts = titles.str.contains(pattern, flags=re.I).sum()
print(python_counts)

—hn_sql = hn[hn['title'].str.contains(r"\w+SQL", flags=re.I)].copy()
hn_sql['flavor']=hn_sql['title'].str.extract(r"(\w+SQL)", re.I)
hn_sql['flavor']=hn_sql['flavor'].str.lower()
sql_pivot=hn_sql.pivot_table(index='flavor',values='num_comments',aggfunc='mean')

— pattern = r"[Pp]ython ([\d\.]+)"

py_versions = titles.str.extract(pattern)
py_versions_freq = dict(py_versions.value_counts())

— def first_10_matches(pattern):
    """
    Return the first 10 story titles that match
    the provided regular expression
    """
    all_matches = titles[titles.str.contains(pattern)]
    first_10 = all_matches.head(10)
    return first_10

pattern = r"\b[Cc]\b[^.+]”				c ya da C olacak ama C++ veya C.E.O olmayacak
first_ten = first_10_matches(pattern)

— pattern = r"(?<!Series\s)\b[Cc]\b(?![\+\.])"		oncesinde ‘Series ‘ (sonda bosluk var) sonrasında da + ve . olmayacak yani ‘Series C’ ve C++ veya C.E.O olmuyor
c_mentions = titles.str.contains(pattern).sum()

— pattern = r"\b(\w+)\s\1\b"			… problem problem …/ … week week …   yani ayni sozcuk ikilediginde

repeated_words = titles[titles.str.contains(pattern)]		

—string = "aBcDEfGHIj"
print(re.sub(r"[A-Z]", "-", string))
a-c--f---j

— sql_variations = pd.Series(["SQL", "Sql", "sql"])

sql_uniform = sql_variations.str.replace(r"sql", "SQL", flags=re.I)
print(sql_uniform)

0    SQL
1    SQL
2    SQL
dtype: object

—email_variations = pd.Series(['email', 'Email', 'e Mail',
                        'e mail', 'E-mail', 'e-mail',
                        'eMail', 'E-Mail', 'EMAIL'])

pattern = r"e[\-\s]?mail"
email_uniform = email_variations.str.replace(pattern, "email", flags=re.I)
titles_clean = titles.str.replace(pattern, "email", flags=re.I)

— test_urls = pd.Series([
 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429',
 'http://www.interactivedynamicvideo.com/',
 'http://www.nytimes.com/2007/11/07/movies/07stein.html?_r=0',
 'http://evonomics.com/advertising-cannot-maintain-internet-heres-solution/',
 'HTTPS://github.com/keppel/pinn',
 'Http://phys.org/news/2015-09-scale-solar-youve.html',
 'https://iot.seeed.cc',
 'http://www.bfilipek.com/2016/04/custom-deleters-for-c-smart-pointers.html',
 'http://beta.crowdfireapp.com/?beta=agnipath',
 'https://www.valid.ly?param',
 'http://css-cursor.techstream.org'
])

pattern = r"https?://([\w\-\.]+)"

test_urls_clean = test_urls.str.extract(pattern, flags=re.I)
domains = hn['url'].str.extract(pattern, flags=re.I)
top_domains = domains.value_counts().head(5)

— pattern = r"(https?)://([\w\.\-]+)/?(.*)"			herbir parantez URL nin bir ismini temsil ediyor

test_url_parts = test_urls.str.extract(pattern, flags=re.I)
url_parts = hn['url'].str.extract(pattern, flags=re.I)		        

— created_at = hn['created_at'].head()

pattern = r"(.+) (.+)"
dates_times = created_at.str.extract(pattern)
print(dates_times)

_          0      1
0   8/4/2016  11:52
1  1/26/2016  19:30
2  6/23/2016  22:20
3  6/17/2016   0:01
4  9/30/2015   4:12

— pattern = r"(?P<date>.+) (?P<time>.+)"
dates_times = created_at.str.extract(pattern)
print(dates_times)

_       date   time
0   8/4/2016  11:52
1  1/26/2016  19:30
2  6/23/2016  22:20
3  6/17/2016   0:01
4  9/30/2015   4:12

— pattern = r"(?P<protocol>https?)://(?P<domain>[\w\.\-]+)/?(?P<path>.*)"
url_parts = hn['url'].str.extract(pattern, flags=re.I)

— As we learned in the previous mission, to extract those mentions, we need to do two things:
	1.	Use the Series.str.extract() method.
	2.	Use a regex capture group.
 
—

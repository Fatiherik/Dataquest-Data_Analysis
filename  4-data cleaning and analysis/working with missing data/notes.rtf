{\rtf1\ansi\ansicpg1252\cocoartf2511
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\froman\fcharset0 Times-Roman;\f2\froman\fcharset0 Times-Italic;
\f3\froman\fcharset0 Times-Bold;\f4\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red41\green46\blue57;\red255\green255\blue255;\red47\green196\blue135;
\red183\green13\blue60;\red247\green237\blue240;}
{\*\expandedcolortbl;;\cssrgb\c21176\c23922\c28627;\cssrgb\c100000\c100000\c100000;\cssrgb\c20000\c80000\c60000;
\cssrgb\c77647\c14118\c30196;\cssrgb\c97647\c94510\c95294;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid1\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid3}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}}
\paperw11900\paperh16840\margl1440\margr1440\vieww28600\viewh15100\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs28 \cf0 Yontem:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl840\partightenfactor0
\ls1\ilvl0
\f1\fs42 \cf2 \cb3 {\listtext	1.	}\expnd0\expndtw0\kerning0
Check for errors in data cleaning/transformation.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl840\partightenfactor0
\ls1\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 {\listtext	2.	}\expnd0\expndtw0\kerning0
Use data from additional sources to fill missing values.\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	3.	}\expnd0\expndtw0\kerning0
Drop row/column.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl840\partightenfactor0
\ls1\ilvl0
\f2\i \cf2 \cb3 \kerning1\expnd0\expndtw0 {\listtext	4.	}\expnd0\expndtw0\kerning0
Fill missing values with reasonable estimates computed from the available data.
\f1\i0 \cb1 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs28 \cf0 \kerning1\expnd0\expndtw0 \
\pard\pardeftab720\sl490\sa280\partightenfactor0

\f1 \cf2 \cb3 \expnd0\expndtw0\kerning0
There are many options for choosing the replacement value, including:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl840\partightenfactor0
\ls2\ilvl0
\fs42 \cf2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
A constant value\cb1 \
\ls2\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
The mean of the column\cb1 \
\ls2\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
The median of the column\cb1 \
\ls2\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
The mode of the column\cb1 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs28 \cf0 \kerning1\expnd0\expndtw0 \
\pard\pardeftab720\sl490\sa280\partightenfactor0

\f1 \cf2 \cb3 \expnd0\expndtw0\kerning0
We also started to set a more defined data cleaning workflow, in which we:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl840\partightenfactor0
\ls3\ilvl0
\fs42 \cf2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Set a goal for the project.\cb1 \
\ls3\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Researched and tried to understand the data.\cb1 \
\ls3\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Determined what data was needed to complete our analysis.\cb1 \
\ls3\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Added columns.\cb1 \
\ls3\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Cleaned specific data types.\cb1 \
\ls3\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Combined data sets.\cb1 \
\ls3\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Removed duplicate values.\cb1 \
\ls3\ilvl0\cb3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Handled the missing values.\
\pard\tx566\pardeftab720\sl840\partightenfactor0
\cf2 \cb1 Projede cevabini aradigin soru/sorular ne?\
Belgeyi okut ve ilk gozatma komutlar\uc0\u305  ile dataya bak.\
Cevaba ula\uc0\u351 mak icin hangi verileri/sutunlari kullanmam gerekiyor? Baska bir tablodan ekstra veri alacak miyim?\
Datalari birle\uc0\u351 tir.\
Gereksiz sutunlari at.\
Sutun eklemen gerekiyorsa ekle.\
Cleaning asamasina gec (duplicate, missing data)\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs28 \cf0 \kerning1\expnd0\expndtw0 \
\
\
\'97 shape_2015=happiness2015.shape\
shape_2016=happiness2016.shape\
shape_2017=happiness2017.shape\
\
\'97missing = happiness2015['Happiness Score'].isnull()	          bos hucreleri Iceren dataframe verir\
happiness2015[missing]\
\
\'97missing_2016= happiness2016.isnull().sum()		herbir kolonda kac bos hucre var\
missing_2017= happiness2017.isnull().sum()\
\
\'97happiness2017.columns = happiness2017.columns.str.replace('.', ' ').str.replace('\\s+', ' ').str.strip().str.upper()\
happiness2015.columns = happiness2015.columns.str.replace('(', '').str.replace(')', '').str.strip().str.upper()\
happiness2016.columns = happiness2016.columns.str.replace('(', '').str.replace(')', '').str.strip().str.upper()\
\
combined = pd.concat([happiness2015, happiness2016, happiness2017], ignore_index=True)\
missing = combined.isnull().sum()\
\
\'97import seaborn as sns\
combined_updated = combined.set_index('YEAR')\
sns.heatmap(combined_updated.isnull(), cbar=False)\
\
\pard\pardeftab720\sl490\sa280\partightenfactor0

\f1 \cf2 \cb3 \expnd0\expndtw0\kerning0
We can learn more about where these missing values are located by visualizing them with a\'a0{\field{\*\fldinst{HYPERLINK "https://seaborn.pydata.org/generated/seaborn.heatmap.html"}}{\fldrslt 
\f3\b \cf4 heatmap}}, a graphical representation of our data in which values are represented as colors. We'll use the seaborn library to create the heatmap.\
\pard\pardeftab720\sl490\partightenfactor0
\cf2 Note below that we first reset the index to be the\'a0
\f4\fs23\fsmilli11900 \cf5 \cb6 YEAR
\f1\fs28 \cf2 \cb3 \'a0column so that we'll be able to see the corresponding year on the left side of the heatmap:\
\

\f0 \'97regions_2017=combined[combined['YEAR']==2017]['REGION']\
missing=regions_2017.isnull().sum().  			 yili 2017 olan dataframedeki region s\'fctununda kac adet bos deger var.\
\
\pard\pardeftab720\partightenfactor0
\cf2 \'97combined=pd.merge(left=combined, right=regions, on='COUNTRY', how='left')\
\pard\pardeftab720\sl490\partightenfactor0
\cf2 combined=combined.drop('REGION_x', axis=1)\
missing=combined.isnull().sum()
\f1 \
\
\'97combined=pd.merge(left=combined, right=regions, on='COUNTRY', how='left')\
combined=combined.drop('REGION_x', axis=1)\
missing=combined.isnull().sum()\
\
\'97dups = combined.duplicated(['COUNTRY', 'YEAR'])		ayni y\uc0\u305 l ve ayni \'fclke var mi\
combined[dups]\
\
\'97combined['COUNTRY']=combined['COUNTRY'].str.upper()\
dups = combined.duplicated(['COUNTRY', 'YEAR'])\
print(combined[dups])\
\
\'97combined['COUNTRY'] = combined['COUNTRY'].str.upper()\
combined=combined.drop_duplicates(['COUNTRY', 'YEAR'])			hangi sutunlara gore silece\uc0\u287 iz, default olarak ilkini b\u305 rak\u305 yor, ikinciyi siliyor, keep=\'91last\'92 dersek tam tersini yapar.	\
\
\'97 columns_to_drop = ['LOWER CONFIDENCE INTERVAL', 'STANDARD ERROR', 'UPPER CONFIDENCE INTERVAL', 'WHISKER HIGH', 'WHISKER LOW']\
combined=combined.drop(columns_to_drop,axis=1)\
missing=combined.isnull().sum()\
\
\'97 df.dropna(axis=1)                  icinde nan deger olan sutunu siler. Default olarak icinde nan deger olan satiri siler.\
\
\'97 combined.notnull().sum().sort_values()\
\
\'97combined=combined.dropna(thresh=159, axis=1)  		 icinde 159 adetten fazla nan deger iceren sutunu siler\
missing=combined.isnull().sum()\
\
\'97happiness_mean=combined['HAPPINESS SCORE'].mean()\
combined['HAPPINESS SCORE UPDATED']=combined['HAPPINESS SCORE'].fillna(happiness_mean)              nan degerleri mean ile doldurma yontemi. nan degerleri mean ile doldurduk ancak sunu farkettik, cogu nan degeri bar\uc0\u305 nd\u305 ran\
print(combined['HAPPINESS SCORE UPDATED'].mean())									subsaharan \'fclkelerinin mean degeri 4,15 ama biz tum \'fclkelerin mean degeri olan 5, 37 soru ile doldurduk. Bunun iyi bir y\'f6ntem 																			olmadigina karar verip, nan degerleri doldurmak yerine sildik.\
\
\'97combined=combined.dropna()\
missing=combined.isnull().sum()\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
}